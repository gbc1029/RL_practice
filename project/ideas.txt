Actor: 
策略：对应状态下选择某动作的概率
 \epsilon -> any  
 (1- \epsilon) * P(a) -> a
 超参数：\epsilon
 超参数：学习率

Critic
值函数：
特征提取：全连接/卷积？
    偏置 + 权重 + 卷积核
求价值：
    特征->价值
    学习对象：样本过程
超参数：
    折扣因子
    学习率
    价值的实际计算方法及参数

特征提取：

预计传入：
config:all
状态：
时序为t
pos:all
speed:all
acelerate:self  (or all?)
动作：
时序t+1
accelerate_new:self

Critic价值估计
输入提取到的特征，输出预估价值

Critic预训练：

得到样本：
    多次模拟MC
    随机动作
    得到状态-动作-价值的样本

    储存：稀疏矩阵
        集合
随机初始化
使用样本训练

Actor初始化：
    平均
状态传入：
    完整状态 or 特征？
